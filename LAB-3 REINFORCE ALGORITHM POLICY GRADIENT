import numpy as np

# Maze Environment
class Maze:
    def __init__(self):
        self.size = 5
        self.start = (0, 0)
        self.goal = (4, 4)
        self.state = self.start

    def reset(self):
        self.state = self.start
        return self.state

    def step(self, action):
        x, y = self.state

        if action == 0:   # up
            x -= 1
        elif action == 1: # down
            x += 1
        elif action == 2: # left
            y -= 1
        elif action == 3: # right
            y += 1

        # Check boundaries
        if x < 0 or x >= self.size or y < 0 or y >= self.size:
            return self.state, -5, False

        self.state = (x, y)

        if self.state == self.goal:
            return self.state, 10, True

        return self.state, -1, False


# REINFORCE Agent
class Agent:
    def __init__(self, state_size, action_size, lr=0.01, gamma=0.99):
        self.state_size = state_size
        self.action_size = action_size
        self.lr = lr
        self.gamma = gamma
        self.theta = np.random.rand(state_size, action_size)

    def get_action(self, state):
        probs = self.policy(state)
        return np.random.choice(self.action_size, p=probs)

    def policy(self, state):
        state_index = state[0]*5 + state[1]
        logits = self.theta[state_index]
        exp = np.exp(logits - np.max(logits))
        return exp / np.sum(exp)

    def update(self, episode):
        G = 0
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = reward + self.gamma * G

            state_index = state[0]*5 + state[1]
            probs = self.policy(state)

            grad = -probs
            grad[action] += 1

            self.theta[state_index] += self.lr * G * grad


# -------- TRAINING --------
env = Maze()
agent = Agent(state_size=25, action_size=4)

episodes = 300
max_steps = 100

for ep in range(episodes):
    state = env.reset()
    episode = []
    done = False
    steps = 0

    while not done and steps < max_steps:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)
        episode.append((state, action, reward))
        state = next_state
        steps += 1

    agent.update(episode)

    if ep % 50 == 0:
        print("Episode", ep, "completed in", steps, "steps")

print("Training Finished!")
